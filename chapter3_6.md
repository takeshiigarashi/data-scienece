<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>


# 線形回帰

**線形回帰**とは、１つ以上の説明変数を利用して、連続値である目的変数を直線モデルで予測する方法。

予測変数$Y$が、説明変数$X$の線形結合で表されるモデル。


## 単回帰分析

**説明変数が１つ**の線形回帰の手法

単回帰分析モデルを表す線形式は以下:

$ Y = aX + b $

ここで、$a$, $b$は**回帰係数**や**パラメータ**、**重み**などと呼ばれる。

## 重回帰分析

**説明変数が複数**

重回帰分析モデルを表す線形式：

$ Y = a_{1}{\cdot}X_{1} + a_{2}{\cdot}X_{2} + {\cdots} + a_{n}{\cdot}X_{n} + b  $

$a_{i} (i = 1, 2, \cdots, n)$: **偏回帰係数**

- **多重共線性（Multicollinearity）** : 相関が強い説明変数（特徴量）を組み合わせた時に、それらが干渉し合うことを という。

## 最小二乗法(Lease Square Method)

以下の$L$が<u>一番小さくなるよう</u>に$f(x)$の係数を求める。

$L = \sum_{i=1}^{N}\{y_{i} - f(x_{i})\}^2$

$L$ は **損失関数 (Loss Function)** という。

線形回帰の場合、回帰係数は損失関数を、$a$と$b$で偏微分した式がともに0となるように方程式を解くことで$a$と$b$が求まる。

## 決定係数

**決定係数**とは、回帰式が実データをどの程度再現しているかを評価するための値。（線形ではないデータ、曲線ようなデータを線形で近似していないかどうかをチェックする）

決定係数$R^{2}$は、以下で定義される。

$R^{2} = 1 - \frac{\sum_{i=1}^{n}[y_i-f(x_i)]^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}$

- 分子は、予測値と実測値の二乗誤差で、予測値の分散を表す。
- 分母は、実測値の平均との二乗誤差で、実測値の分散になっている。（分散の式は、さらに$n$で割る）

決定係数$R^2$は0から1の間をとり、**1に近いほど良いモデル**（1に近いほど回帰式がデータとの重なりが大きくなる）。

## 正則化

損失関数に**ペナルティ項**を追加することで、過学習を防ぐ。

正則化では、損失関数にペナルティ項を追加した次の式を最小化する。

$E(\bm{w}) + \lambda\frac{1}{p}\sum_{i}|w_i|^p)$

ここで、パラメータ$\lambda$は、ペナルティの重さを制限する役割を持っている。$\lambda$を大きくすると過学習に陥りにくくなるが、逆に学習不足になることもある。


- **L1正則化**: $p=1$とする  
特徴選択と次元圧縮に効果的
- **L2正則化**: $p=2$とする  
パラメータの大きさをゼロに近づける効果があり、汎化性の高いモデルが得られる。（過学習防止に効果的）
- **ラッソ回帰**: L1正則化を取り入れた線形回帰のこと
- **リッジ回帰**: L2正則化を取り入れた線形回帰のこと
- **Elastic Net**: ラッソ回帰とリッジ回帰を組み合わせた手法



