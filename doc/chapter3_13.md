<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 3.13 教師なし機械学習

- クラスタリング（クラスター分析）
  - 階層クラスター分析
  - 非階層クラスター分析
- 次元削減

## クラスタリング（クラスター分析）

類似性を持つデータ同士をグループに分類する方法。

正解データは与えられないため、データの特徴に基づいてグループに分ける。

|   |階層クラスター分析|非階層クラスター分析
|---|---|---
|手法|デンドログラム|K平均法
|事前のクラスタ数|不要|必要
|初期値への依存|ない|初期値に依存し、局所最適解の問題などがある
|データ量が多い場合|不向き|向いている


## 階層クラスター分析

データ集合から最も近いデータを順にグルーピング。 **デンドログラム（樹形図）** では、距離を分岐点までの高さで表す。適切な高さで線を引くことで、データをクラスター（グループ）に分ける。

階層化されているため説明しやすい。ブランドや商品など視点を定めて顧客情報をクラスター化するなど。

**データ数が多すぎると、テンドログラムが複雑になり、クラスターを決めにくくなる**

### 距離の計算方法

**ユークリッド距離** ２点間の直線距離。

$$d(\bm{x}, \bm{y}) = \sqrt{(x_1 - y_1)^2 + \cdots + (x_n - y_n)^2}$$

**マンハッタン距離** 都市のブロックのように垂直や水平での移動のみ可能な場合の距離を測定する。

$$d(\bm{x}, \bm{y}) = |x_i - y_i| + \cdots + |x_n - y_n|$$

**ミンコフスキー距離** 

$$d(\bm{x}, \bm{y}) = \Big( \sum_{i=1}^n |x_i - y_i|^p \Big)^{\frac{1}{p}}$$

- $p=1$ のとき、マンハッタン距離になる。
- $p=2$ のとき、ユークリッド距離になる。

**マハラノビス距離** データの分布を考慮して、異なるスケールや相関のあるデータに対して適切な距離を測定する。

$$d(\bm{x}, \bm{y}) = \sqrt{(\bm{x} - \bm{y})^T \bm{S}^{-1}(\bm{x}-\bm{y})}$$

ここで、 $\bm{S}^{-1}$ は、 $\bm{x}$ と $\bm{y}$ が属するデータセットの共分散行列の逆行列。マハラノビス距離は、背景となるデータセットや分布がないと計算ができないため、ベクトルが属するデータセットが必要となる。

### クラスタ同士を結びつける際に使用する連結法

- **最短距離法** 最短距離を使う
- **郡平均法** 平均距離を使う
- **重心法** 重心を使う
- **メディアン法** 中央値を使う
- **ウォード法** クラスタリング過程でクラスタ間の分散の増加を最小化する手法。分散分析に基づいた手法で、クラスタを結合する際に、結合後のクラスタ内のデータ点の分散が最も小さくなるようにクラスタを選ぶ。（分散分析については 8.12 に説明を追加した）

## 非階層クラスター分析

**あらかじめ決めておいた数のクラスターにデータを分離**する。

代表的な手法は**K-means (K 平均法)**。

- Kはユーザーが指定する
- データをランダムにK個のクラスターに分ける
- 各クラスターのデータ点の**重心**を求める
- 各データ点とK個の重心の距離を計算する
- K個の重心のうち、**距離が一番近い重心**を含むクラスターに各データを割り当て直す
- 上記を、**重心の位置が変化しなくなるまで**繰り返す

高速かつ大規模データの分析に適している。SNSの投稿を分類するなど。


## 次元削減と主成分分析

**主成分分析(PCA: Principal Component Analysis)** は、次元削減の代表的な手法。

例：国語、数学、理科、社会、英語という５教科の点数（５次元のデータ）を、「理系力」と「文系力」という２つの軸で評価する（２次元のデータ）。

相関のある多くの変数を、相関が少ない合成変数（主成分）に要約する。

### 次元削減と教師あり学習の併用

学習データに対して次元削減を施すことで、精度を向上させることができる。

## 局所最適解の問題

学習アルゴリズムが最適な解（グローバル最適解）に到達せず、途中の不完全な解（局所最適解）で停止してしまう問題を指す。機械学習全般にあてはまる問題だが、K平均法にも同様の問題がおこる可能性がある（階層クラスター分析では起こらない）。

初期値の違いによってえられる解が異なるため、再現性や一貫性のある結果を得られないという問題もある。
