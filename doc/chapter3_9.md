<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 3.9 決定木

特徴量（説明変数）を軸に複数のグループに分割し、最下段の各ノードがなるべく同じ属性（同じクラス）のデータのみで構成されるような分割軸としきい値を探す。

分類（目的変数が質的データ）で用いられるものを分類器と呼ぶが、連続値を扱う回帰器もある。

学習は、**ノード内の不純度を最大限減らす**ように、分割軸としきい値が最適化されていく。不純度は、**ジニ不純度**や**エントロピー**を評価指標として用いる。不純度やエントロピーを用いて、分岐前のノードの不純度から分岐後のノードの不純度を引いた値を**情報利得**として定義すると、分岐によって不純度が下がることで、情報利得は大きくなる（<u>ジニ不純度やエントロピーが小さくなると、情報利得が大きくなるように定義されている</u>）。学習では、情報利得が大きくなるようにしきい値を探していく。

モデルの構造とデータ処理がシンプルだが、**過学習しやすい**。外れ値やノイズの影響を受けやすく、データ分割に偏りが生じやすい。**データ量が少ないのに特徴量の数が多いときに起こりやすい**。

## アンサンブル学習器

**アンサンブル学習器**とは、<u>単純なモデル（弱学習器）を多数組み合わせて使うことで精度を改善する手法</u>。**バギング**と**ブースティング**という手法がある。

- **バギング**: 弱学習器を多数作り、並列に学習されたあと、すべての結果を結合する。（例：**ランダムフォレスト**）
- **ブースティング**: 弱学習器を逐次的に作ることで、以前に構築された弱学習器の結果を用いる。（例：**勾配ブースティング木**)

**ランダムフォレスト**

- データと特徴量の両方をランダムにサンプリング
- データは、重複を許すサンプリングを行う（**ブートストラップ法**）
- 特徴量は、全特徴量が $M$ 個なら、 $\sqrt{M}$ 個程度を抽出する
- 分類問題の場合は多数決、回帰問題の場合は予測値の平均値を出力する。

ランダムフォレストは、並列処理ができるため高速。実装しやすく、同時に高い精度を見込める。

**勾配ブースティング**

- １つの弱学習器モデルを作成し学習する
- 最初の学習器で誤識別したデータに重みを増やして、次の弱学習器ではその部分を重点的に学習する。
- 上記を繰り返す
- XGBoost, AdaBoost, lightGBMなどがある

ブースティングはチューニングが難しく、機械学習上級者向け。学習に時間がかかる。

一方で、高い精度をだすこともでき、KaggleやSignateなどのコンペでは人気がある。

分類木は、多種多様な特徴量を扱う柔軟性があるので好まれる。

## 決定木モデルの強み

- <u>構造化データ</u>を扱うため、<u>特徴量が扱いやすく、モデルによるデータ処理の仕組みが直感的にわかりやすい</u>
- 決定木の仕組みをツリー構造で可視化するツールもある。
- ランダムフォレストでは、特徴量の重要度を可視化するすることができる。（説明がしやすい）


## ジニ不純度、エントロピー、情報利得の式

### ジニ不純度

$t$ 番目のノードの**ジニ不純度**は、次の式で与えられる。

$$\text{I}_\text{G}(t) = 1 - \sum_{i=1}^c p(i|t)^2$$

ここで、 $c$ は全クラスの数、 $p(i|t)$ はノード $t$ 内に属するクラス $i$ のデータの割合。

「融資しない」と「限度額３０万円」の２クラスがあり、ノード $t$ において、それぞれ40件、10件だった場合、ジニ不純度は以下のように計算する。

$$\text{I}_\text{G}(t) = 1 - \Big( \frac{40}{50}^2 + \frac{10}{50}^2 \Big) = 1 - 0.68 = 0.32$$

融資しないが50件、限度額３０万円が0件になると、ジニ不純度は0になる。（小さくなる）

### エントロピー

$t$ 番目のノードの**エントロピー**は、次の式で与えられる。

$$\text{I}_\text{H}(t) = - \sum_{i=1}^c p(i|t) \log_2 p(i|t)$$

### 情報利得

上記の不純度を用いて、**情報利得**は、次の式で与えられる。

$$\Delta\text{I}_\text{G} = \text{I}_\text{G}(t_B) - w_L\text{I}_\text{G}(t_L) - w_R\text{I}_\text{G}(t_R)$$

ここで、 $t_B$ は分岐前のノード、 $t_L$ 、 $t_R$ は分岐後の左右の２つのノード、 $w_L$ 、 $w_R$ は分岐後のノードの重み。

情報利得は、分岐前の不純度から分岐後の不純度を引いた値になっているため、分岐後の不純度が小さくなるほど、情報利得の値は大きくなる。



