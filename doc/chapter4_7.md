<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 4.7 大規模言語モデル

- Transformer ... 文章解析用のニューラルネットワーク
- BERT ... Googleから公開された大規模言語モデル
- GPT ... OpenAIから公開された大規模言語モデル
- 基盤モデル

## 事前学習とファインチューニング

- 事前学習は、教師なし学習として実行される。  
インターネット上の大量のテキストデータを学習。自動的に統計的なパターンや意味を抽出する。
- コーパス ... 学習用のテキストデータ。数十億件のトークンに及ぶ

事前学習を終えた学習済みモデル（ニューラルネットワーク）は、隠れ層で、多くの文章に共通する汎用的な特徴量を習得した状態になる。  
新しい言語タスクへの**転移学習**に利用することができる。

**ファインチューニング**は、事前学習済みのモデルのパラメータを調整することで、特定のタスクに適用できるようにすること。特定のタスクに特化したデータセットを一定量用意し、主にネットワークの下流の部分のパラメータのみ、追加学習と値の更新を行う。

事前学習が教師なしであったのに対して、ファインチューニングは教師あり学習。通常、ファインチューニングに必要なデータは数百〜数千と、事前学習に比べると少ない。

## スケール則

**モデルのパラメータ数と訓練データの計算量が増加するにつれて、モデルの性能もほぼ同じ割合で向上する**という経験則

## GPTモデル

Generative Pre-trained Transformer の略。

GPTの事前学習は、与えられた文章（単語系列）の次に来るべき単語を予測し、文を自動完成できるように基礎訓練されている。事前学習は教師なし学習（自己教師学習）を行う。ランダムな一で文章の後半を隠し（マスキング）、前半部分から後半の単語を当てる穴埋め問題を大量に解くことでパラメータが最適化される。

GPTは、数少ない事例を与えるだけで、次に来るべき単語を逐次的に予測しながら、文章を自動的に完成できることが特徴。人間に近い自然な文章を生成できる。

GPTにファインチューニングを適用することによって、文章の生成だけでなく、翻訳、質疑応答、校正、ブレインストーミング、自然言語からソースコードを生成するなど様々なタスクに使用することができる。

GPTは基盤モデル、ChatGPTはGPTを基盤モデルとした対話型の文章生成AI。

##　大規模言語モデルの背景にある技術

Transformer以前は、**RNN（リカレントニューラルネットワーク、再帰型ニューラルネットワーク）** が自然言語処理に用いられていた。RNNは時系列データを扱えることを特徴としたニューラルネットワーク。RRNには「フィードバック」という仕組みを持つことで、過去の隠れ層と現在の隠れ層に繋がりを持っている。こうすることで、過去の情報に基づいて、前の時刻の情報を記憶し、それらを考慮しながら現時刻の情報を処理することができる。

２つのRNNモデルを繋いだモデルを**Sequence-to-Sequence(Seq2Seq)** という。時系列データを入力して処理し、時系列データを出力することからこの名前がついている。

RNNを用いた言語モデルには課題があった。

- 入力系列が名が唸ると、遠く離れた単語間の関係や文脈を正しく把握できなくなる
- 入力データを１ステップごとに処理する必要があり、並列処理ができない

2017年、Googleの研究チームが開発した**Transformer**の登場により、上記の問題が改善され、長い文章の解析精度が飛躍的に向上した。背景にあるのは、内部にもつ**Attention機構**が重要な役割を担っている。

RNNは、過去のある時刻の情報が現在の予測に"どれだけ影響するか"を算出することができない。これに対して、Attention機構は、入力された系列の各単語が、他のすべての単語とどの程度関連しているかを計算する機能を持っている。重要度の高い情報に注意を向けて学習できるようになっている。


