<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 3.11 ナイーブベイズ

ナイーブベイズは、与えられたデータから各クラスの確率を計算し、**事後確率**が最も高いクラスに観測データを分類する、という手法。

例えば、メールのスパム判定や良い知らせか悪いしらせかを、文章中に含まれる単語の出現頻度を使って推測する。

使用例

- リアルタイム予測 
- テキスト分類
- レコメンド 
- センチメント分析

## 無相関の仮定

ナイーブベイズでは、説明変数が独立して予測対象に影響を与えていると仮定する。説明変数同士は互いに無相関であると仮定される。

## ベイズの定理

**ベイズの定理**

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

この式自体は条件付き確率の定義から簡単に出てくるが、事前確率、事後確率、原因、結果などの捉え方をしっかり理解する必要がある。


### ベイズの定理を用いた確率の計算で使う公式

ベイズの定理を用いた確率の計算では、以下の公式を使うことがある（特に、右辺分母の $P(B)$ を計算するときに使う）。

$$P(A) = P(A|B)P(B) + P(A|B^c)P(B^c)$$

上式は、確率の加法定理と全確率の法則から導かれる。

**確率の加法定理**:

２つの事象 $A$ と $B$ に対して、
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

が成り立つ。
ここで、 $A$ と $B$ が互いに背反である場合、 $P(A \cap B)$ は0になるから、

$$P(A \cup B) = P(A) + P(B)$$

となる。

事象 $B$ とその余事象 $B^c$ を用いると、 $A = (A \cap B) \cup (A \cap B^c)$ であるから、

$$P(A) = P((A \cap B) \cup (A \cap B^c))$$

である。ここで、 $B$ と $B^c$ は互いに背反であるから、 $A \cap B$ と $A \cap B^c$ もまた互いに背反である。そのため、

$$P(A) = P((A \cap B) \cup (A \cap B^c)) = P(A \cap B) + P(A \cap B^c)$$

が成り立つ。これを、一般化すると、全確率の法則が得られる。

**全確率の法則**:

事象 $B_1, B_2, \cdots, B_n$ が、互いに背反で、その和事象が全事象を構成する場合、 $P(A)$ は、 $B_1, B_2, \cdots, B_n$ を用いて次のように表せる。

$$P(A) = \sum_{i=1}^n P(A \cap B_i)$$

更に、条件付き確率の定義を用いると、

$$P(A) = \sum_{i=1}^n P(A|B_i)P(B_i)$$

と書ける。

### 余事象を用いた全確率の法則

事象 $A$ と、事象 $B, B^c$ に対して全確率の法則を適用すれば、以下が成り立つ。

$$P(A) = P(A \cap B) + P(A \cap B^c) = P(A|B)P(B) + P(A|B^c)P(B^c)$$


### 具体例

病気の検査の例がイメージしやすい。

- A：病気に罹患している（これを「原因」の事象と捉える）
- B：検査結果が陽性であった（これを「結果」の事象と捉える）

ベイズの定理は、以下のようにかける

$$P(\text{ 病気 }|\text{ 陽性 }) = \frac{P(\text{ 陽性 }|\text{ 病気 }) \cdot P(\text{ 病気 })}{P(\text{ 陽性 })}$$

ここで、左辺は、「検査結果が陽性だったときに、実際に病気に罹患している確率」を表している。左辺の確率を直接的に計算することは難しいが、右辺に出てくるそれぞれの確率があらかじめ算出されていれば、左辺の確率が計算できる。

https://www.tech-teacher.jp/blog/statistics_4_conditional/ の例を参考にして、具体例を示す。

罹患率が0.1% (0.001)の病気の検査を考える。
この検査では、陽性、陰性が以下のように判定されることがわかっているとする。

- 病気に罹患している人に対して陽性とでる確率が95%、陰性とでる確率が5%
- 病気に罹患していない人に対して陽性とでる確率が20%、陰性とでる確率が80%

表にすると以下になる。

|                   |陰性  |陽性 |
|-------------------|:-----:|:-----:|
| 病気に罹患していない | 80%  | 20% |
| 病気に罹患している   | 5%   | 95% |

ここで、陽性と判定されたときに実際に病気に罹患している確率を求めたい。

求めたいのはベイズの定理の左辺の値になる。そのため、右辺に出てくる確率を事前にわかっている情報から求める。

$P(\text{ 陽性 } | \text{ 病気 }) = 0.95$

$P(\text{ 病気 }) = 0.001$

$P(\text{ 陽性 })$ を計算するときに、全確率の法則を用いる。全確率の法則で、$B = \{病気に罹患している\}$ 、 $B^c = \{病気に罹患してない\}$ とおいて、全確率の法則を用いると、

$$P(\text{ 陽性 }) = P(\text{ 陽性 } | \text{ 病気 })P(\text{ 病気 }) + P(\text{ 陽性 }|\text{ 病気でない })P(\text{ 病気でない })$$

すなわち、以下で求められる。

$$\begin{aligned}
P(\text{ 陽性 }) \\
& = P(\text{ 陽性 } | \text{ 病気 })P(\text{ 病気 }) + P(\text{ 陽性 }|\text{ 病気でない })P(\text{ 病気でない })\\
& = 0.95 \times 0.001 + 0.2 \times (1 - 0.001)\\
& \simeq 0.2
\end{aligned}$$

最終的に求めたい確率は、

$$\frac{0.95 \times 0.001}{0.2} = 0.00475$$

結果としては 0.475% になる。


### 事前確率、事後確率

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

ベイズに定理において、 $P(A|B)$ を**事後確率**、 $P(A)$ を**事前確率**、 $P(B|A)$ は**尤度**という。

BのもとでAが起こる確率を、AのもとでBが起こる確率やA、Bの確率から計算できる、というのがベイズの定理。

病気の検査の例だと、検索結果が陽性のときに罹患している確率を、罹患しているときに陽性である確率や、罹患している確率、陽性である確率などから求めている。

### ナイーブベイズ分類器への応用

https://qiita.com/kazuya_minakuchi/items/c3a0066e90cfdfc8a859


文書内に含まれる単語の出現頻度からカテゴリを分類するタスクを考える。

カテゴリはAとBの２つのみとする。

あらかじめ、以下のデータが与えられているとする。

|出現する単語|カテゴリ|
|----------|-------|
|a,b,c     | A |
|a,c       | A |
|c         | A |
|a         | A |
|a,b       | B |
|a         | B |
|b         | B |

このとき、

出現単語が a のみである記事のカテゴリを推定したい。
つまり、出現単語が a のみであるとき、カテゴリがAである確率とカテゴリがBであるときの確率を計算し、確率が高い方を推定値としたい。

求めたい確率は以下の２つ

- $P[A | (a, \bar{b}, \bar{c})]$ ... 出現する単語がaのみであるとき、カテゴリがAである確率 ... (1)
- $P[B | (a, \bar{b}, \bar{c})]$ ... 出現する単語がaのみであるとき、カテゴリがBである確率 ... (2)

まず、１つ目の確率を考える。
ベイズの定理から、１つ目の確率は以下で計算できる。

$$P[A | (a, \bar{b}, \bar{c})] = \frac{P[(a,\bar{b},\bar{c})|A] \cdot P[A]}{P[(a,\bar{b},\bar{c})]}$$

$P[(a,\bar{b},\bar{c})|A]$は、ナイーブベイズの前提としている説明変数の独立性より、 $P[a|A]$ , $P[\bar{b}|A]$ , $P[\bar{c}|A]$ の積で計算してよい。

$P[(a,\bar{b},\bar{c})|A]\\
= P[a|A] \cdot P[\bar{b}|A] \cdot P[\bar{c}|A]\\
= \frac{3}{4} \cdot (1 - \frac{1}{4}) \cdot (1 - \frac{3}{4})$

$P[A]$は、カテゴリがAである確率で、 $4 / 7$ である。

$P[(a,\bar{b},\bar{c})]$は、計算しなくてよい（(1)と(2)の両方に出てくるが、値は定数となるため、比較するときは無視して良い）

上記から、 $P[A | (a, \bar{b}, \bar{c})]$ は以下になる

$P[A | (a, \bar{b}, \bar{c})] = \frac{4}{7} \cdot \frac{3}{4} \cdot (1 - \frac{1}{4}) \cdot (1 - \frac{3}{4}) \simeq 0.080$

同様に、 $P[B | (a, \bar{b}, \bar{c})]$ を計算すると$0.094$になる。

Bである確率の方が高いため、カテゴリとしてはBと推定する。

