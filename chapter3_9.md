<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# 3.9 決定木

特徴量（説明変数）を軸に複数のグループに分割し、最下段の各ノードがなるべく同じ属性（同じクラス）のデータのみで構成されるような分割軸としきい値を探す。

分類（目的変数が質的データ）で用いられるものを分類器と呼ぶが、連続値を扱う回帰器もある。

学習は、**ノード内の不純度を最大限減らす**ように、分割軸としきい値が最適化されていく。不純度は、**ジニ不純度**や**エントロピー**を評価指標として用いる。

モデルの構造とデータ処理がシンプルだが、**過学習しやすい**。外れ値やノイズの影響を受けやすく、データ分割に偏りが生じやすい。**データ量が少ないのに特徴量の数が多いときに起こりやすい**。

## アンサンブル学習器

**アンサンブル学習器**とは、<u>単純なモデル（弱学習器）を多数組み合わせて使うことで精度を改善する手法</u>。**バギング**と**ブースティング**という手法がある。

- **バギング**: 弱学習器を多数作り、並列に学習されたあと、すべての結果を結合する。（例：**ランダムフォレスト**）
- **ブースティング**: 弱学習器を逐次的に作ることで、以前に構築された弱学習器の結果を用いる。（例：**勾配ブースティング木**)

**ランダムフォレスト**

- データと特徴量の両方をランダムにサンプリング
- データは、重複を許すサンプリングを行う（**ブートストラップ法**）
- 特徴量は、全特徴量が$M$個なら、$\sqrt{M}$個程度を抽出する
- 分類問題の場合は多数決、回帰問題の場合は予測値の平均値を出力する。

ランダムフォレストは、並列処理ができるため高速。実装しやすく、同時に高い精度を見込める。

**勾配ブースティング**

- １つの弱学習器モデルを作成し学習する
- 最初の学習器で誤識別したデータに重みを増やして、次の弱学習器ではその部分を重点的に学習する。
- 上記を繰り返す
- XGBoost, AdaBoost, lightGBMなどがある

ブースティングはチューニングが難しく、機械学習上級者向け。学習に時間がかかる。

一方で、高い精度をだすこともでき、KaggleやSignateなどのコンペでは人気がある。

分類木は、多種多様な特徴量を扱う柔軟性があるので好まれる。

## 決定木モデルの強み

- <u>構造化データ</u>を扱うため、<u>特徴量が扱いやすく、モデルによるデータ処理の仕組みが直感的にわかりやすい</u>
- 決定木の仕組みをツリー構造で可視化するツールもある。
- ランダムフォレストでは、特徴量の重要度を可視化するすることができる。（説明がしやすい）







