<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# ニューラルネットワーク

多くの場合、画像や音声などの非構造化データの分類に利用される。

**入力層**、**隠れ層**、**出力層**で構成される。分類問題に用いる場合、出力層には**予測クラス**の数だけノードがあり、各ノードからの出力値は該当クラスにデータが属する確率を表す。

例：０から９までの数字が手書きで書かれた画像を識別（分類）する場合出力層は10個のノードがあり、各ノードは、画像がその数字である確率を表す。

- **全結合層**: 隠層のニューロンが前後の層すべてのノードとエッジで結合されていること。
- **単純パーセプトロン**: 構成要素の最小単位
- **多層パーセプトロン**: 単純パーセプトロンをたくさんつなぎあわせたもの
- **活性化関数**: 各ニューロンで、入力データ（１つ前の層の数だけある）と出力データ（１つの数値）を対応付ける関数。

## 単純パーセプトロン

- 入力層と出力層の２層のみのシンプルな構造。
- 出力層のノードは0か1の二値のみ出力。
- **線形分離可能な問題にしか使えない** (活性化関数が線形結合で表現されるため、直線や平面で分離できる問題にしか使えない)

## 多層パーセプトロン(MLP)

- 入力層と出力層の他に、**中間層（隠れ層）**を持つ、３層以上のパーセプトロン
- 活性化関数に非線形関数（シグモイド関数などと思う）を用いることで、複数クラスの分類と線形分離不可能な問題にも対応可能

## ディープニューラルネットワーク(DNN)

ディープニューラルネットワークは多層パーセプトロンと構造的には似ているが、隠れ層の数が多いニューラルネットワークを指す。使われる活性化関数もMLPと異なるものが使われることがある。

|  |MLP|DNN|
|--|---|---|
|定義|少なくとも１層の隠れそうを持つフィードフォワード型のニューラルネットワーク|MLPと構造は似ているが、一般的には「深い」層構造を持つニューラルネットワーク|
|構造|全結合層で構成される|MLPと同じだが、層が深くなることで複雑なパターンをより細かく表現できるようになる|
|活性化関数|非線形活性化関数（ReLUやシグモイドなど）を使って、層間で非線形変換を行う|より深い層で学習を当ていさせるため、ReLUやLeaky ReLUなどが多く用いられる|
|層数|１〜数層程度|通常5層以上を持つ|
|用途|比較的単純な分類や回帰タスク。画像認識には限界がある|画像認識、自然言語処理、音声認識などに用いられている|

**ディープラーニング**は、DNNを含む多層構造のニューラルネットワークを用いて複雑なパターンや特徴を学習するための機械学習の一分野。ディープラーニングはDNNだけでなく、**CNN**、**RNN**、**生成モデル**などを含む。

DNNはディープラーニングの手法の１つ。ディープラーニングは、DNNを含む深層モデルを学習・運用するためのアルゴリズム、技術、応用など全体を指す。例えば、訓練データの前処理、ハイパーパラメータの調整、GPUによる高速化、モデルの評価などの要素もディープラーニングに含まれる。

## 重みとバイアス

ニューロン間の結合の強さを線形結合で表現する。

$x = w_1x_1 + w_2x_2 + \cdots + w_Nx_N + b$

ここで、$(w_1, w_2, \cdots, w_N)$を**重み**、$b$をバイアスという。

## 活性化関数

重みとバイアスで線形結合された値を、**活性化関数**にわたすことで、出力が決定される。

活性化関数としては、シグモイドやReLUなどが使われる。
なお、活性化関数にシグモイドを使うと、ロジスティック回帰と同じ式になる。

## 誤差逆伝播と勾配消失問題

一般的な機械学習の最適化プロセスと同様、ニューラルネットワークでも出力データと正解ラベルの誤差を小さくするようパラメータを最適化していく。つまり、**損失関数を最小化し、最適解に近づけるために、ネットワークの角層の重みやバイアスを繰り返し調整する**のがニューラルネットワークにおける学習になる。

入ラルネットワークの最適化は、出力層に近い層から更新されていく。これを **誤差逆伝播 (Back Propagation)** と呼ぶ。

**勾配消失問題**とは、この誤差逆伝播を計算する仮定で、**勾配**に関する情報が小さくなっていき、伝播ができなくなり、学習が進まなくなる問題。
**勾配**とは、**勾配降下法**において、各パラメータに対して損失関数の値がどのように変化するかを示す**微分の値**。勾配の値が大きい場合、パラメータを大きく変化させて損失を減らす方向へ進むことができる。勾配の値が小さいと、パラメータの更新量が小さくなり、学習が遅くなる。層を遡るたびに勾配が小さくなっていくことがある。入力層に近い層では勾配が非常に小さくなり、重みがほとんど変化しなくなるため、学習が進まなくなる。このことを「勾配消失」という。

活性化関数にReLUやその変種は、勾配がゼロになりにくく、勾配消失を軽減する。その他にもいくつかの対処法がある。
